{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introduction Prob Stats #\n",
    "\n",
    "### Primers on Probability and Statistics ###\n",
    "\n",
    "For a random variable $X$, we define the mean $\\mu$ as the expectation, $E(X)$.\n",
    "\n",
    "Expectation operator is defined as:\n",
    "\n",
    "$$\n",
    "E(X) = \\int_{-\\infty}^{\\infty} x f(x) dx\n",
    "$$\n",
    "\n",
    "where $f(x)$ is the density function of the random variable. The expectation operator is linear in its arguments. If given a sample of size $n$, an unbiased estimator of the expected value $\\bar{x}$ is:\n",
    "\n",
    "$$\n",
    "\\bar{x} = n^{-1} \\sum_{i}^n x_i .\n",
    "$$\n",
    "\n",
    "By saying it is unbiased, we are saying $E(\\bar{X}) = E (X) = \\mu$.\n",
    "\n",
    "We next define the Variance:\n",
    "\n",
    "$$\n",
    "V(X) = \\sigma^2 = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x) dx = E\\left(\\left(X - E(X)\\right)^2\\right).\n",
    "$$\n",
    "\n",
    "A property of the variance, is that for 2 constants $a$ and $b$, and 2 random variables $X$ and $Y$\n",
    "\n",
    "$$\n",
    "V(aX + bY) = a^2 V(X) + b^2 V(Y) + 2 a b Cov(X,Y)\n",
    "$$\n",
    "\n",
    "where $Cov(X,Y)$ is the covariance between the 2 random variables,\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = E\\left((X-\\mu_X)(Y-\\mu_Y)\\right).\n",
    "$$\n",
    "\n",
    "Note that an unbiased estimator for the variance is:\n",
    "\n",
    "$$\n",
    "s^2 = (n-1)^{-1} \\sum_i^n (x_i - \\bar{x})^2.\n",
    "$$\n",
    "\n",
    "To see where we get the $(n-1)^{-1}$ comes from, calculate the expected difference between the true variance and the estimator with only $n^{-1}$:\n",
    "\n",
    "$$\n",
    "E(\\sigma^2 - s_n^2) = E\\left[\\frac{1}{n}\\sum_i^n(x_i -\\mu)^2 - \\frac{1}{n}\\sum_i^n(x_i -\\bar{x})^2\\right]\\\\\n",
    "= \\frac{\\sigma^2}{n}.\n",
    "$$\n",
    "\n",
    "This can be generalized. The $k$th central moment is defined as:\n",
    "\n",
    "$$\n",
    "\\mu_k = E\\left((X-\\mu)^k\\right).\n",
    "$$\n",
    "\n",
    "Skewness is the third central moment, kurtosis is the fourth central moment. There are many different definitions of sample skewness and kurtosis to make them unbiased, especially for small samples.\n",
    "\n",
    "The normal distribution has 0 skewness and 3 kurtosis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11725329018321863\n",
      "0.3325138867811026\n",
      "0.005586148793054809\n",
      "-1.2005744528917148\n"
     ]
    }
   ],
   "source": [
    "'''Basic demonstration in python'''\n",
    "\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "# Random Data between -100 and 100\n",
    "data = (np.random.rand(10000)-0.5)*200\n",
    "#Function to calculate moments about mean for sample\n",
    "samplemean = np.average(data)\n",
    "samplevar = scipy.stats.moment(data,moment=2)/(len(data)-1)\n",
    "sampleskew = scipy.stats.skew(data)\n",
    "samplekurt = scipy.stats.kurtosis(data)\n",
    "print(samplemean)\n",
    "print(samplevar)\n",
    "print(sampleskew)\n",
    "print(samplekurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Distributions ###\n",
    "\n",
    "#### Binomial Distribution ####\n",
    "\n",
    "This distribution describes the distribution of results of an experiment where there are only 2 outcomes, and where each trial is independent. If p is the results of obtaining a success,\n",
    "\n",
    "$$\n",
    "P(X=m) = {N \\choose m} p ^m (1-p)^{N-m}, \\\\\n",
    "E(X) = Np, \\\\\n",
    "V(X) = Np(1-p).\n",
    "$$\n",
    "\n",
    "#### Poisson Distribution ####\n",
    "\n",
    "The Poisson distribution describes the frequency of events during a fixed time interval, the the chance of each event occuring is independent. This can be derived from the Binomial distribution. We let $\\lambda t = n p$, where $\\lambda$ is a parameter describing the expected number of events per unit time, and $t$ is the time. This describes a scenario where the expected number of successes in $\\lambda t$ is given by the binomial mean $n p$. We now substitute p out of the expressions for the binomial distribution, and take the limit $n \\rightarrow \\infty$. This describes the scenario where the chance of success $p$ goes to $0$ and the number of trials goes to infinity, while keeping the expected mean of successes constant. This gives:\n",
    "\n",
    "$$\n",
    "P(X=x) = \\frac{\\exp(-\\lambda t)(\\lambda t)^x}{x!}.\n",
    "$$\n",
    "\n",
    "This distribution implies that the waiting time distribution between successes associated with a Poisson process is exponential. We note that $P(X=0) = \\exp(-\\lambda t)$, which is the probability of having no successes before time $t$. The probability of having a success before time $t$ is thus $1-\\exp(-\\lambda t)$. This in other words is the continuous random variable corresponding to the waiting time between events. By differentiating it, we get the associated density function $f(t) = \\lambda \\exp (-\\lambda t)$.\n",
    "\n",
    "#### Uniform Distribution ####\n",
    "\n",
    "This is a distribution equal anywhere in the range.\n",
    "\n",
    "#### Normal Distribution ####\n",
    "\n",
    "This is a distibution with the density function:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(- \\frac{(x-\\mu )^2}{2 \\sigma^2}\\right).\n",
    "$$\n",
    "\n",
    "If the random variable $X$ is normally distributed with expectation $\\mu$ and standard deviation $\\sigma$, we denote it as $X \\sim N(\\mu,\\sigma^2)$. Any normal random variable can be transformed into the standard normal distribution with mean 0 and standard deviation 1 by using the transform $Z = \\frac{X-\\mu}{\\sigma}$.\n",
    "\n",
    "#### Stable Distributions ####\n",
    "\n",
    "A distribution is stable if a linear combination of two independent random variables with that distribution has the same distribution. A normal distribution is stable, which is extremely useful in typical analysis. For example, if $X$ and $Y$ are two normal distributions, $X+Y$ is also a normal distribution. Other than the normal distribution, the other stable distributions with closed form are the Cauchy distribution and the Levy distribution.\n",
    "\n",
    "#### Lognormal Distribution ####\n",
    "\n",
    "This is a distibution with the density function:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{x\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(- \\frac{(\\ln (x)-\\mu )^2}{2 \\sigma^2}\\right).\n",
    "$$\n",
    "\n",
    "One useful property of this distribution is that $\\exp (X)$ is a lognormal distribution if and only if $X$ is a normal distribution. This makes it very useful for modelling scenarios like the case where we know the return is a normal random variable, and thus the asset price itself will be a lognormal distribution. \n",
    "\n",
    "#### More Exotic Distributions ####\n",
    "\n",
    "Usually, while we like to work with normal distributions, they are usually not very accurate for modelling real financial data as they tend to be more skewed and leptokurtic than the normal distribution. One method is to combine mixtures of normal distributions $f$:\n",
    "\n",
    "$$\n",
    "g(x) = a f_1(x) + (1-a) f_2(x).\n",
    "$$\n",
    "\n",
    "For such a mixture of 2 normal distributions, the resulting distribution will have a greater kurtosis than the normal distribution of the same variance.\n",
    "\n",
    "One other distribution we note here is the Student t distribution, with a parameter called the `degree of freedom' $\\nu$:\n",
    "\n",
    "$$\n",
    "f_v(t) - (\\nu \\pi)^{-1/2} \\Gamma (\\nu/2)^{-1} \\Gamma ((\\nu+1)/2) (1 + t^2/\\nu)^{(\\frac{\\nu + 1}{2})},\n",
    "$$\n",
    "\n",
    "where $\\Gamma$ is the gamma function. Again, one useful property of this distribution is that it has a leptokurtic character.\n",
    "\n",
    "There are many other types of distributions used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Distributions ###\n",
    "\n",
    "#### Bivariate Distributions ####\n",
    "\n",
    "A bivariate density function is a function that is integrable, never negative, and is normalized.\n",
    "We first define one such distribution, the joint distribution function $F(x,y)$:\n",
    "\n",
    "$$\n",
    "F(x,y) = P(X<x,Y<y) = \\int_{- \\infty}^y\\int_{- \\infty}^x f(x,y) dx dy, \\\\\n",
    "f(x,y) = \\frac{\\partial^2 F(x,y)}{\\partial x \\partial y}.\n",
    "$$\n",
    "\n",
    "We also define the marginal distribution\n",
    "\n",
    "$$\n",
    "H(x) = F(x,\\infty),\\\\\n",
    "G(y) = F(\\infty,y).\n",
    "$$\n",
    "\n",
    "The marginal densities\n",
    "\n",
    "$$\n",
    "h(x) =H^\\prime (x)= \\int_{- \\infty}^{\\infty} f(x,y) dy,\\\\\n",
    "g(y) =H^\\prime (y)= \\int_{- \\infty}^{\\infty} f(x,y) dx.\n",
    "$$\n",
    "\n",
    "Also, the conditional distribution $F(x|y)$ denotes the distribution of $Y$ given that $X$ takes a fixed value.\n",
    "\n",
    "$$\n",
    "F(x|y) = \\frac{\\partial F(x,y)}{\\partial x}.\n",
    "$$\n",
    "\n",
    "The conditonal density\n",
    "\n",
    "$$\n",
    "f(x|y) = \\frac{f(x,y)}{g(y)}.\n",
    "$$\n",
    "\n",
    "#### Independent Random Variables ####\n",
    "\n",
    "Two variables $X$ and $Y$ are independent if and only if $F(x,y) = H(x)G(y)$. One way of looking at this is that the conditional distributions for $X$ are all the same, and equal to the marginal distribution $H(x)$. The density functions obey a similar rule, $f(x,y) = h(x)g(y)$.\n",
    "\n",
    "#### Covariance ####\n",
    "\n",
    "The covariance was described earlier. It is the first central moment of the joint density function of $X$ and $Y$:\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = E\\left((X-\\mu_X)(Y-\\mu_Y)\\right).\n",
    "$$\n",
    "\n",
    "It is also given by:\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = E(XY) - E(X)E(Y).\n",
    "$$\n",
    "\n",
    "In parameter notation:\n",
    "\n",
    "$$\n",
    "\\sigma_{XY} = \\mu_{XY} - \\mu_X \\mu_Y.\n",
    "$$\n",
    "\n",
    "This is very important, because of a property discussed above:\n",
    "\n",
    "$$\n",
    "V(aX + bY) = a^2 V(X) + b^2 V(Y) + 2 a b Cov(X,Y).\n",
    "$$\n",
    "\n",
    "If $X$ and $Y$ are independent, then their covariance and correlation will be 0.\n",
    "\n",
    "#### Correlation ####\n",
    "\n",
    "This is a standardized form of covariance, independent of units of measurement\n",
    "\n",
    "$$\n",
    "Corr(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{V(X)V(Y)}}.\\\\\n",
    "\n",
    "Corr(aX,bY) = \\begin{cases} Corr(X,Y) \\text{ if } ab>0\\\\ -Corr(X,Y) \\text{ if } ab<0 \\end{cases}.\n",
    "\n",
    "$$\n",
    "\n",
    "#### Multivariate Normal Distribution ####\n",
    "\n",
    "This describes a scenario where there might be multipel normal distributions with non-zero pairwise correlations. Suppose there are $k$ variables $\\vec{x}$ with expectations $\\vec{\\mu}$, and a symmetric covariance matrix \n",
    "$$\n",
    "\\hat{V}_{ij} = \\begin{cases} \\sigma_i^2 \\text{ if } i=j \\\\ \\sigma_i \\sigma_j \\text{ otherwise } \\end{cases}.\n",
    "$$\n",
    "\n",
    "The multivariate normal density function is given as:\n",
    "\n",
    "$$\n",
    "\\phi(\\vec{x}) = (2 \\pi)^{-k/2} |\\hat{V}|^{-1/2} \\exp(-\\frac{1}{2} (\\vec{x} - \\vec{\\mu})^T \\hat{V}^{-1} (\\vec{x} - \\vec{\\mu})),\n",
    "$$\n",
    "\n",
    "and we write $\\vec{X} \\sim N_k(\\vec{\\mu}, \\hat{V})$ for the vector random variable.\n",
    "\n",
    "One important property is that every portfolio $R$ containing these assets $R = \\sum_i \\omega_i x_i $ is a normal distribution $R\\sim N(\\mu, \\sigma^2), \\mu = \\vec{\\omega}^T\\vec{\\mu}, \\sigma^2 = \\vec{\\omega}^T \\hat{V} \\vec{\\omega}$.\n",
    "\n",
    "When two random variables have a bivariate normal distribution they are independent if and only if their correlation is 0. More generally, if $X$ and $Y$ are bivariate normal variables with correlation $\\rho_{XY}$ then $E(Y|X) = \\mu_y + \\rho_{XY} \\frac{\\sigma_x}{\\sigma_y} (X-\\mu_x)$, $V(Y|X) = \\sigma_y^2(1 - \\rho^2)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a26e4b7189bf03eb81522675f6e6c88684ba722547c6ad443d3676f771c332f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
