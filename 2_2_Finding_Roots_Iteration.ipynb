{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Roots through Iteration #\n",
    "\n",
    "\n",
    "Usually, we are interested in finding roots of a non-linear equation $f(x)=0$. They are typically tackled with numerical methods using iterative algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method of Bisection ####\n",
    "\n",
    "This is the simplest algorithm, but is slow. We usually start by finding 2 values of $x$, $x_1$ and $x_2$, such that $f(x_1)$ and $f(x_2)$ have different signs. Set $I = [x_1,x_2]$. We bisect the interval $I$ by taking the midpoint and evaluating the function at that point. After which, we replace the interval with another that has half the width. Below is an image of the process taken from wikipedia.\n",
    "\n",
    "![Imagefromwikipedia](images/2_2_bisection.png)\n",
    "\n",
    "One can see that the interval decreases by half after every iteration.\n",
    "\n",
    "Usually, the function $f$ is called the *objective function*, and most algorithms have a *tolerance level* that is related to the *criterion for convergence*. For an algorithm that finds the roots of the equation, it will be deemed to have converged to the solution $x=x^*$ when $|f(x^*)| < T $, where $T$ is the tolerance level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton-Raphson Iteration ####\n",
    "\n",
    "The previously mentioned method of bisection has a *linear convergence*, which means that the precision of the solution is approximately equal to the number of steps used in the iteration. Typically, we want to look for methods that have faster convergence, for example *quadratic convergence*. To be more precise, in quadratic convergence, the error at the $(n+1)$ th iteration is approximately equal to the square root of the error at the $n$ th iteration. The Newton-Raphson iteration method has quadratic convergence.\n",
    "\n",
    "The idea is to start from the definition of the derivative (and thus the tangent) of the function:\n",
    "\n",
    "$$\n",
    "f^\\prime (x_n) = \\frac{f(x_n)-f(x_{n+1})}{x_n - x_{n+1}}.\n",
    "$$\n",
    "\n",
    "Now, let us approximate the function with its tangent line. If so, the root of the approximated function is the point $x_{n+1}$ where the tangent cuts the x-axis, thus $f(x_{n+1})=0$. This is usually not going to be the true root of the function, but it provides a nearer approximation of the root.\n",
    "\n",
    "The iterative scheme is described as:\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f^\\prime (x_n)}.\n",
    "$$\n",
    "\n",
    "Below is a picture from wikipedia illustrating the proceedure.\n",
    "\n",
    "![picturefromwikipedia](images/2_2_newton_raphson_wikipedia.png)\n",
    "\n",
    "One can easily use the solvers in scipy that use the Newton Raphson method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000269\n"
     ]
    }
   ],
   "source": [
    "def f(x): #A test function\n",
    "    return x**3 - 1 \n",
    "\n",
    "initial_guess = 2\n",
    "\n",
    "print(optimize.newton(f,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Methods ####\n",
    "\n",
    "One can also apply the Newton-Raphson method to find the stationary points of the function ($f^\\prime = 0$). This is done by applying the iteration to the first derivative of the function.\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f^\\prime (x_n)}{f^{\\prime \\prime}(x_n)}.\n",
    "$$\n",
    "\n",
    "This does require that the fuction is twice continuously differentiable, and that the second derivative is nowhere 0, for it to converge.\n",
    "\n",
    "One can observe how to generalize methods like this. Let us assume $f(\\vec{x})$ is a function of $m$ variables. Let us define the vector $\\vec{g}, g_i = \\partial_i f$, and the matrix $H, h_{ij} = \\partial_i \\partial_j f$. These are the gradient vector of first partial derivatives and the Hessian matrix of second partial derivatives. If the Hessian is invertible at the point $\\vec{x}_n$, we can now speficy the iterative scheme, called *Newtons method*:\n",
    "\n",
    "$$\n",
    "\\vec{x}_{n+1} = \\vec{x}_n - H^{-1}(\\vec{x}_n) \\vec{g}(\\vec{x}_n).\n",
    "$$\n",
    "\n",
    "This is an example of a more general class of optimization algorithms known as *gradient methods*. The simplest gradient method is of the form:\n",
    "\n",
    "$$\n",
    "\\vec{x}_{n+1} = \\vec{x}_n - s \\vec{g}(\\vec{x}_n),\n",
    "$$\n",
    "\n",
    "where $s$ is a positive constant called the *step length*. This step length could be variable, meaning that we can choose to take smaller steps when the gradient is large, and larger steps when the gradient is small. However, this method still has only linear convergence. One way to improve the convergence rate is to use:\n",
    "\n",
    "$$\n",
    "\\vec{x}_{n+1} = \\vec{x}_n - \\Theta_n \\vec{g}(\\vec{x}_n),\n",
    "$$\n",
    "\n",
    "where $\\Theta_n$ is a judiciously chosen matrix. It can be seen that Newtons method (with the inverse of the Hessian matrix) is a special case of this. One can even make a modification of Newtons method with a variable step length.\n",
    "\n",
    "Even though these methods have quadratic convergence, the conditions for convergence are quite strong. Firstly, the Hessian needs to be invertible everywhere. Secondly, the calculation of the Hessian and its inverse can be extremely costly. Alternatives that sacrifice accuracy for speed are the method of *conjugate gradients* and *quasi-Newton methods*.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "316e860a238c8892b0bc89caf8795812aa72b12f46a5ef18699767900e41eca6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
