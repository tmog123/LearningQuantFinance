{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Introduction Statistical Inference #\n",
    "\n",
    "When a random variable $X$ has a known distribution function $F(x)$ we can find the quantiles of $X$ by inverting this function. For example, the lower $0.05$ quantile of $X$ is given by $F^{-1}(0.05)$, which is a point that cuts off the lower $5\\%$ of the distribution.\n",
    "\n",
    "$$\n",
    "P(X<F^{-1}(\\alpha)) = \\alpha.\n",
    "$$\n",
    "\n",
    "When $X$ is a sampling random variable, the upper and lower percentiles of $X$ are called its critical values. For a standard normal distribution symmetric about zero, this is easily calculated on python. This is also the inverse of the cumulative distribution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5758293035489004\n",
      "0.995\n"
     ]
    }
   ],
   "source": [
    "a = scipy.stats.norm.ppf(0.995)\n",
    "print(a)\n",
    "print(scipy.stats.norm.cdf(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval is a range where one is confident the true parameter lies to a certain extent. So, $(A,B)$ is a $95\\%$ confidence interval for $\\theta$ if $P(A<\\theta <B) = 0.95$. These confidence intervals with an upper and lower parameter are known as two-sided confidence intervals. We can also consider one-sided confidence intervals, where one of the parameters is $\\infty$ or $-\\infty$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Central Limit Theorem ####\n",
    "\n",
    "Suppose we have a sequence of independent and identically distributed random variables $\\{X_1,\\dots,X_n\\}$ with finite expectation $\\mu$ and finite variance $\\sigma^2$. The expectation value of the mean is $E(\\bar{X}_n)=\\mu$, and variance $V(\\bar{X}_n) = n^{-1}\\sigma^2$. The standard error of the sample mean is $\\sigma / \\sqrt{n}$. We consider what is the shape of the distribution of the sample mean. Since normal distribution are stable, the sum of $n$ independent normal random variables is another normal variable. \n",
    "\n",
    "In the cases where the underlying population is not normally distributed, the Central Limit Theorem states that the distribution of the sample mean will converge to a normal distribution as $n\\rightarrow \\infty$. Put another way,\n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} P \\left( \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}} < z \\right) = \\Phi(z),\n",
    "$$\n",
    "\n",
    "where $\\Phi(z)$ is the value of the standard normal distribution at $z$. This can be used to help us construct approximate two-sided confidence intervals for an unknown population mean. \n",
    "\n",
    "$$\n",
    "P(\\bar{X}_n - \\Phi^{-1}(\\frac{1+\\alpha}{2})\\times \\frac{\\sigma}{\\sqrt{n}} < \\mu < \\bar{X}_n + \\Phi^{-1}(\\frac{1+\\alpha}{2})\\times \\frac{\\sigma}{\\sqrt{n}}  ) = \\alpha.\n",
    "$$\n",
    "\n",
    "However, this does not help us estimate $\\sigma$, which is needed in the calculations of $\\Phi^{-1}$.\n",
    "\n",
    "#### Confidence intervals based on Student t Distribution ####\n",
    "\n",
    "We replace the true variance $\\sigma$ with an estimated variance. This causes $\\bar{X}_n$ to become t distributed. We thus replace\n",
    "\n",
    "$$\n",
    "P \\left( \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}} < x \\right) \\approx t_{n-1}(x),\n",
    "$$\n",
    "\n",
    "where $t_{n-1}(x)$ is the value of the standard Student $t$ distribution with $n-1$ degrees of freedom. This then allows us to construct the approximate confidence interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.7933779732323292, -1.9208985718108291, 0.3341426253461708)\n"
     ]
    }
   ],
   "source": [
    "# Function to return mean, and confidence interval\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "# Random Data between -100 and 100\n",
    "data = (np.random.rand(10000)-0.5)*200\n",
    "\n",
    "print(mean_confidence_interval(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence intervals for Variance ####\n",
    "\n",
    "Confidence intervals for variance $\\sigma^2$ are constructed in a similar way, but using the chi-squared distribution instead\n",
    "\n",
    "$$\n",
    "\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi_{n-1}^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.336296988922984, 0.32716629365160477, 0.3458171728999795)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.distributions import chi2\n",
    "\n",
    "# Function to return variance, and confidence interval\n",
    "def var_confidence_interval(data,confidence=0.95):\n",
    "    samplevar = scipy.stats.moment(data,moment=2)/(len(data)-1)\n",
    "    n = len(data)\n",
    "    alpha = 1-confidence\n",
    "    df = n - 1 \n",
    "    lower = (n - 1) * samplevar / chi2.ppf(1 - alpha / 2, df)\n",
    "    upper = (n - 1) * samplevar / chi2.ppf(alpha / 2, df)\n",
    "    return samplevar, lower,upper\n",
    "\n",
    "# Random Data between -100 and 100\n",
    "data = (np.random.rand(10000)-0.5)*200\n",
    "\n",
    "print(var_confidence_interval(data))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Tests ####\n",
    "\n",
    "The framework is:\n",
    "\n",
    "1) Set up the null $H_0$ and alternative $H_1$ hypotheses.\n",
    "\n",
    "2) State the test statistic $X$.\n",
    "\n",
    "3) Choose a significance level.\n",
    "\n",
    "4) Determine the critical region. This will depend on the test statistic $X$ and the chosen significance level.\n",
    "\n",
    "5) Evaluate the test statistic.\n",
    "\n",
    "6) Apply the decision rule.\n",
    "\n",
    "We illustrate this with a few examples.\n",
    "\n",
    "### Test on Means ###\n",
    "\n",
    "Assume that we draw a sample from a population with a normal distribution, but we do not know the expectation nor variance of the distribution. We wish to test $H_0:\\mu = 0$ versus $H_1:\\mu<0$. The test statistic is:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{X}_n - \\mu}{s} \\sim t_{n-1}.\n",
    "$$\n",
    "\n",
    "Suppose we take a sample of size 10, observe a mean 2.5, and a variance $s^2 = 2.5$. Then the value of the test statistic under the null hypothesis is $5$. The one-sided critical region for a test of size $0.05$ is $(-\\infty,-1.83)$. Thus, in this scenario, we cannot reject the null hypothesis in favor of the one-sided alternative $\\mu<0$. However, we could reject the null in favour of the alternative $\\mu>0$ at a very high level of significance.\n",
    "\n",
    "Since the t distribution converges to a standard normal distribution if the sample is large, we use the normal distribution for the test, and this is called a Z test.\n",
    "\n",
    "Now suppose we have two samples, and we want to decide whether they are both drawn from the same population. We can test for the equality of two means using the t statistic:\n",
    "\n",
    "$$\n",
    "\\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}} \\sim t_{n_1 + n_2 -2}.\n",
    "$$\n",
    "\n",
    "### Test on Variances ###\n",
    "\n",
    "Similar methods can be used to test for variances. However, the test statistic $X$ now has a chi-squared distribution with $n-1$ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation ###\n",
    "\n",
    "MLE is a general method for estimating the parameters of a distribution. It is used extensively as the estimators are consistent, meaning the distribution converges to the true parameter as the sample size increases. The likelihood of a sample $\\vec{x} = (x_1,\\dots,x_n)$ is the product of the values of the density function at each observation $x_i$. The likelihood is given as\n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta} | \\vec{x}) = \\prod_{i}^n f(x_i,\\vec{\\theta}).\n",
    "$$\n",
    "\n",
    "Usually, we do not optimize this directly. Instead, we work with the log of the likelihood function. This is valid as log is a monotonic increasing function.\n",
    "\n",
    "As an example of how this might work, we take a probability density function $\\phi$ with expectation $\\mu$ and $\\sigma$.\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{(x-\\mu)^2}{2 \\sigma^2}  \\right) \\\\\n",
    "-2 \\ln \\phi(x) = \\text{constant} + \\ln(\\sigma^2) + \\frac{(x-\\mu)^2}{\\sigma^2}.\n",
    "$$\n",
    "\n",
    "Now, if we are given a sample $\\vec{x} = (x_1,\\dots,x_n)$, maximizing the likelihood is equivalent to minimizing $-2$ times the log likelihood, which is minimizing\n",
    "\n",
    "$$\n",
    "-2 \\sum_i^n \\ln \\phi(x_i).\n",
    "$$\n",
    "\n",
    "If we differentiate this with respect to $\\mu$ and $\\sigma^2$, we obtain the conditions\n",
    "\n",
    "$$\n",
    "\\sum_i^n (x_i - \\mu) = 0\\\\\n",
    "\\sum_i^n (x_i - \\mu)^2 = n \\sigma^2.\n",
    "$$\n",
    "\n",
    "This gives you the estimators\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}=\\bar{x} = \\frac{1}{n} \\sum_i^n x_i \\\\\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_i^n (x_i-\\bar{x})^2.\n",
    "$$\n",
    "\n",
    "If you consider the Hessian matrix of second order partial derivatives, we realize it is positive definite, so the stationary point found is a local minimum.\n",
    "\n",
    "One can also calcualte the standard errors of the maximum likelihood estimates. The information matrix $I(\\sigma)$ is the matrix of expected values of the negative of the second derivatives of the log likelihood. The inverse of the information matrix is the covariance matrix of the maximum likelihood estimator. From the diagonals of the matrix, you can then obtain the standard errors of the maximum likelihood estimators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a26e4b7189bf03eb81522675f6e6c88684ba722547c6ad443d3676f771c332f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
