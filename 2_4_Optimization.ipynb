{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization #\n",
    "\n",
    "This is the process of finding a maximum or minimum of a function. One can add in constraints, which makes it a constrained optimization problem.\n",
    "\n",
    "#### Least Squares Problems ####\n",
    "\n",
    "Many statistical optimization problems involve changing parameters of an objective function such that the function is able to fit a given set of data as closely as possible. The least squares problem is a part of this class of problems, where the objective function measures the mean square error of the actual data and the fitted data.\n",
    "\n",
    "The linear regression done in part 1.4 is a special case of the least squares problem.\n",
    "\n",
    "Let $\\vec{y}$ denote the data that we are aiming to fit, and $\\vec{\\theta}$ the parameters that we can change. $\\vec{f}(\\vec{\\theta}|\\vec{x})$ is the function that we are trying to fit with $\\vec{y}$. We can now define the error function as:\n",
    "\n",
    "$$\n",
    "\\vec{\\epsilon}(\\vec{\\theta}|\\vec{x}) = \\vec{y} - \\vec{y}(\\vec{\\theta}|\\vec{x}).\n",
    "$$\n",
    "\n",
    "Now the general least squares objective function is:\n",
    "\n",
    "$$\n",
    "\\min_{\\vec{\\theta}} (\\vec{\\epsilon}^\\dag(\\vec{\\theta}| \\vec{x}) \\vec{\\epsilon}(\\vec{\\theta}| \\vec{x}) ) \\text{ s.t. } \\vec{h} (\\vec{\\theta}) \\leq 0,\n",
    "$$\n",
    "\n",
    "where $\\vec{h} (\\vec{\\theta})$ encodes the constraints on the values of the parameters. Unlike the examples discussed in part 1.4, when the function is not linear, it is typically not able to obtain analytical solutions. In the general case we minimize this by applying an iterative gradient method. One popular algorihtm for this is the *Levenberg-Marquardt algorithm*. This is of the form:\n",
    "\n",
    "$$\n",
    "\\vec{\\epsilon}(\\vec{\\theta}_{n+1}) = \\vec{\\epsilon}(\\vec{\\theta}_{n}) + J(\\vec{\\theta}_n)(\\vec{\\theta}_{n+1} - \\vec{\\theta}_n).\n",
    "$$\n",
    "\n",
    "In this, $J$ is the Jacobian matrix of partial derivatives, $j_{ij} = \\frac{\\partial f_i}{\\partial \\theta_j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood methods ####\n",
    "\n",
    "Maximizing a likelihood function is a very flexible method for estimating the parameters of any distribution, and *maximum likelihood estimation* (MLE) is a standard procedure for doing so. Typically, algorithms for optimizing the likelihood function are more complex than least squares algorithms, and gradient methods will not always converge. The likelihood can be a very flat surface, making it extremely hard to converge on a global optimum. Thus, one needs to take more care when using a maximum likelihood proceedure.\n",
    "\n",
    "In general, the covariance matrix of the estimators is given as:\n",
    "\n",
    "$$\n",
    "V(\\vec{\\theta}) = - I(\\vec{\\theta})^{-1}, I(\\vec{\\theta})^{-1} = E\\left[ \\frac{\\partial^2 \\log L}{\\partial\\vec{\\theta} \\partial\\vec{\\theta}^\\prime } \\right].\n",
    "$$\n",
    "\n",
    "$I(\\vec{\\theta})$ is called the information matrix. For a normal density, it is diagonal, but it can be a complicated non-diagonal matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM Algorithm ####\n",
    "\n",
    "The *expectation-maximization* (EM) algorithm assumes that there are hidden variables or parameters that cannot be observed directly in the data. For example, assume the dataset $\\vec{x}$ is generated from 2 distributions. If we randomly pick any single data point $x$, we cannot tell which distribution it originated from. We can only say that it was generated from a distribution with a certain probability. Indeed, this probability is usually what we intend to estimate. We call the hidden variable that indicates which distribution it comes from as a *latent variable*.\n",
    "\n",
    "The EM algorithm is designed to explicitly include latent variables in the maximization of the likelihood. Denote the latent variable is $y$. The algorithm consists of iterating between two steps, the E-step and the M-step. The E-step is the creation of a function for the expectation of the expected log likelihood, given the current estimates of $\\vec{\\theta}$, which will give some distribution for $h(y)$. Then, the M-step invovles applying an optimization method to find new values of $\\vec{\\theta}$ that maximizes the expected log likelihood.\n",
    "\n",
    "1) Initialize the parameters $\\vec{\\theta}$ to some random values.\n",
    "\n",
    "2) Calculate $h(y)$ given $\\vec{\\theta}$.\n",
    "\n",
    "3) Use $h(y)$ to obtain a better estimate for $\\vec{\\theta}$\n",
    "\n",
    "4) Iterate steps 2 and 3 until convergence.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a26e4b7189bf03eb81522675f6e6c88684ba722547c6ad443d3676f771c332f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
